{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Client Setup [local]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installa la AWS CLI scaricandola da questo url: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html#getting-started-install-instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una volta installata, incolla le informazioni dalla Command line or programmatic access nella folder .aws nel profilo utente di sistema.\n",
    "\n",
    "Nel file config\n",
    "```json\n",
    "[username]\n",
    "region = us-west-2\n",
    "output = json\n",
    "```\n",
    "\n",
    "Nel file credential inserire le informazioni in questo formato:\n",
    "\n",
    "```json\n",
    "[username]\n",
    "aws_access_key_id=\n",
    "aws_secret_access_key=\n",
    "aws_session_token=\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-west-2\"  # E.g. \"us-east-1\"\n",
    "os.environ[\"AWS_PROFILE\"] = \"[fillthis]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-west-2\n",
      "  Using profile: sky\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-west-2.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from utils import bedrock\n",
    "from langchain.llms import Bedrock\n",
    "\n",
    "\n",
    "bedrock_runtime = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    runtime=True\n",
    ")\n",
    "\n",
    "model = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\", \n",
    "    client=bedrock_runtime,\n",
    "    model_kwargs={'temperature': 0.5}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Client Setup [AWS LAB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from langchain.llms import Bedrock\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "\n",
    "bedrock_runtime = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    runtime=True\n",
    ")\n",
    "\n",
    "model = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\", \n",
    "    client=bedrock_runtime,\n",
    "    model_kwargs={'temperature': 0.5}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test del client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Ecco la mia poesia sulla città di Napoli:\\n\\nNapoli bella, città del sole\\nTra il mare blu e il Vesuvio che tuona\\nLe tue strade piene di vita e rumore\\nIl tuo dialetto che scalda il cuore\\n\\nPizza margherita, sfogliatella e babà  \\nProfumi e sapori che incantano già\\nMandolini e canzoni napoletane\\nRiempiono l'aria di note lontane\\n\\nIl tuo golfo splendido circonda  \\nLa tua storia antica e profonda\\nTra vicoli stretti e palazzi maestosi\\nNapoli mia, sei proprio preziosa\\n\\nCon arte e cultura ti distingui\\nE un calore umano che sempre distingui\\nNapoli amata, Napoli mia\\nSei tu la regina della melodia\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Componi una poesia sulla città di Napoli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain mette a disposizione una interfaccia agnostica rispetto al LLM che decidiamo di utilizzare. \n",
    "I metodi utilizzabili sono:\n",
    "- stream\n",
    "- invoke\n",
    "- batch\n",
    "\n",
    "Ci sono anche i corrispondenti metodi async che saranno noti a chi conosce python. Per ora li lasceremo da parte.\n",
    "\n",
    "Questi metodi possono essere utilizzati tramite il LangChain Expression Language (LCEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Componi una poesia sulla seguente città: {city}\")\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error raised by bedrock service: An error occurred (ExpiredTokenException) when calling the InvokeModel operation: The security token included in the request is expired",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\EGRIMAAY3\\OneDrive - NTT DATA EMEAL\\Desktop\\antonio\\progetti\\2023\\sky\\workshop\\aws-workshop-genai\\.venv\\Lib\\site-packages\\langchain_community\\llms\\bedrock.py:434\u001b[0m, in \u001b[0;36mBedrockBase._prepare_input_and_invoke\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 434\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    436\u001b[0m     text, body \u001b[38;5;241m=\u001b[39m LLMInputOutputAdapter\u001b[38;5;241m.\u001b[39mprepare_output(\n\u001b[0;32m    437\u001b[0m         provider, response\n\u001b[0;32m    438\u001b[0m     )\u001b[38;5;241m.\u001b[39mvalues()\n",
      "File \u001b[1;32mc:\\Users\\EGRIMAAY3\\OneDrive - NTT DATA EMEAL\\Desktop\\antonio\\progetti\\2023\\sky\\workshop\\aws-workshop-genai\\.venv\\Lib\\site-packages\\botocore\\client.py:553\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\EGRIMAAY3\\OneDrive - NTT DATA EMEAL\\Desktop\\antonio\\progetti\\2023\\sky\\workshop\\aws-workshop-genai\\.venv\\Lib\\site-packages\\botocore\\client.py:1009\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m   1008\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[1;32m-> 1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mClientError\u001b[0m: An error occurred (ExpiredTokenException) when calling the InvokeModel operation: The security token included in the request is expired",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRoma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\EGRIMAAY3\\OneDrive - NTT DATA EMEAL\\Desktop\\antonio\\progetti\\2023\\sky\\workshop\\aws-workshop-genai\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2053\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2052\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[1;32m-> 2053\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   2056\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2058\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2059\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2060\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2061\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\EGRIMAAY3\\OneDrive - NTT DATA EMEAL\\Desktop\\antonio\\progetti\\2023\\sky\\workshop\\aws-workshop-genai\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:235\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    232\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    233\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 235\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    246\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\EGRIMAAY3\\OneDrive - NTT DATA EMEAL\\Desktop\\antonio\\progetti\\2023\\sky\\workshop\\aws-workshop-genai\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:530\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    524\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    528\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    529\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\EGRIMAAY3\\OneDrive - NTT DATA EMEAL\\Desktop\\antonio\\progetti\\2023\\sky\\workshop\\aws-workshop-genai\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:703\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    687\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    688\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    689\u001b[0m         )\n\u001b[0;32m    690\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    691\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    692\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    701\u001b[0m         )\n\u001b[0;32m    702\u001b[0m     ]\n\u001b[1;32m--> 703\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\EGRIMAAY3\\OneDrive - NTT DATA EMEAL\\Desktop\\antonio\\progetti\\2023\\sky\\workshop\\aws-workshop-genai\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:567\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    566\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    568\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32mc:\\Users\\EGRIMAAY3\\OneDrive - NTT DATA EMEAL\\Desktop\\antonio\\progetti\\2023\\sky\\workshop\\aws-workshop-genai\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:554\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    546\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    551\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    553\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 554\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[0;32m    558\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    561\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    562\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    563\u001b[0m         )\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    565\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\EGRIMAAY3\\OneDrive - NTT DATA EMEAL\\Desktop\\antonio\\progetti\\2023\\sky\\workshop\\aws-workshop-genai\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1139\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1136\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1138\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1139\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1140\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1141\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1142\u001b[0m     )\n\u001b[0;32m   1143\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32mc:\\Users\\EGRIMAAY3\\OneDrive - NTT DATA EMEAL\\Desktop\\antonio\\progetti\\2023\\sky\\workshop\\aws-workshop-genai\\.venv\\Lib\\site-packages\\langchain_community\\llms\\bedrock.py:708\u001b[0m, in \u001b[0;36mBedrock._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    705\u001b[0m         completion \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m completion\n\u001b[1;32m--> 708\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_input_and_invoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\EGRIMAAY3\\OneDrive - NTT DATA EMEAL\\Desktop\\antonio\\progetti\\2023\\sky\\workshop\\aws-workshop-genai\\.venv\\Lib\\site-packages\\langchain_community\\llms\\bedrock.py:441\u001b[0m, in \u001b[0;36mBedrockBase._prepare_input_and_invoke\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    436\u001b[0m     text, body \u001b[38;5;241m=\u001b[39m LLMInputOutputAdapter\u001b[38;5;241m.\u001b[39mprepare_output(\n\u001b[0;32m    437\u001b[0m         provider, response\n\u001b[0;32m    438\u001b[0m     )\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 441\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError raised by bedrock service: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    444\u001b[0m     text \u001b[38;5;241m=\u001b[39m enforce_stop_tokens(text, stop)\n",
      "\u001b[1;31mValueError\u001b[0m: Error raised by bedrock service: An error occurred (ExpiredTokenException) when calling the InvokeModel operation: The security token included in the request is expired"
     ]
    }
   ],
   "source": [
    "chain.invoke({\"city\":\"Roma\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\",\"Sei un cantante adori comppor canzoni ma odi comporre poesie. Se l'utente ti chiede di comporre poesie rispondi in modo sgarbato\"),\n",
    "    (\"human\",\"Componi una poesia riguardo la seguente città: {city}\")\n",
    "]\n",
    "prompt = prompt = ChatPromptTemplate.from_messages(messages=messages)\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'city': {'title': 'City', 'type': 'string'}}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'BedrockOutput', 'type': 'string'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.output_schema.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream\n",
    "Invoca la generazione e ritorna i chunck man mano che sono generati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\",\"Sei poeta che si ispira al Dolce Stil Novo\"),\n",
    "    (\"human\",\"Componi una poesia riguardo la seguente città: {city}\")\n",
    "]\n",
    "prompt = prompt = ChatPromptTemplate.from_messages(messages=messages)\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ecco la mia umile proposta di poesia in stile Dolce Stil Novo su Biella:\n",
      "\n",
      "O nobile città di Biella bella\n",
      "che sorgi tra le Alpi a testa alta, \n",
      "le tue vie lastricate di pietra arenaria\n",
      "conducono al tuo cuore antico.\n",
      "\n",
      "Le acque del Cervo e dell'Oropa\n",
      "ti bagnano delicatamente,\n",
      "mentre il Piazzo ti sovrasta\n",
      "con le sue torri medievali.\n",
      "\n",
      "I tuoi telai da secoli tessono\n",
      "preziose stoffe di lana e cachemire,\n",
      "frutto dell'ingegno e del lavoro\n",
      "di generazioni di abili artigiani.\n",
      "\n",
      "Tra i palazzi nobiliari\n",
      "si affacciano cortili fioriti,\n",
      "che profumano di gelsomino e rosmarino\n",
      "in questa primavera ormai imminente.\n",
      "\n",
      "O Biella, città operosa e gentile,\n",
      "accoglimi tra le tue"
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"city\": \"Biella\"}):\n",
    "    print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke\n",
    "Invoca LLM con i parametri inviati e restituisce la risposta solo a fine elaborazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Eccomi, nobile poeta, a comporre versi in stile dolce e nuovo su Milano:\\n\\nO Milano gentile, città industriosa\\nche sorgi altera in mezzo alla pianura.  \\nLe tue torri svettano verso il cielo, \\nsimbolo di operosità e ingegno.\\n\\nLe vie brulicano di gente indaffarata,  \\nmentre il Duomo veglia maestoso e antico.\\nTra i palazzi e le piazze si respira \\nun'aria fresca di novità e progresso. \\n\\nO Milano, tu sei fucina di talenti,\\ncrocevia di culture, culla di bellezza.\\nNel tuo grembo fervono arte e scienza,\\ne tutto il mondo ammira la tua grandezza.\\n\\nCon affetto ti canto, città laboriosa,\\nche guardi al futuro con passo sicuro.\\nRicca di storia, eppur sempre in movimento,\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"city\": \"Milano\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch\n",
    "Permette di invocare la stessa chain in parallelo su parametri diversi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" Ecco una poesia in stile Dolce Stil Novo su Napoli:\\n\\nNapoli mia, città del cuore\\nDove amore e passione ardono in core.\\nTra vicoli stretti e antichi palazzi  \\nSi respira profumo di sfogliatelle e mozzarelle.\\n\\nIl tuo golfo blu come zaffiro\\nRiflette il sole e il tuo bel sguardo.\\nIl Vesuvio ti veglia da lontano\\nSimbolo del tuo animo vulcanico.\\n\\nLa tua voce è musica pura  \\nChe riempie l'aria di gioia e dolcezza.\\nSei tu la sirena che mi ha stregato\\nNapoli, sei tu la mia felicità.\",\n",
       " \" Eccomi, signora mia, a comporre per voi una poesia nello stile del Dolce Stil Novo, dedicata alla città eterna di Roma:\\n\\nO nobile Roma, città senza tempo, \\nche nel tuo grembo racchiudi tesori,\\ndi gloria e potere un giorno fosti centro,\\nora dimora d'artisti e sognatori.\\n\\nLe tue vie lastricate parlano al cuore,\\ni Fori e il Colosseo narrano gesta antiche,\\nla luce del tramonto dona calore\\nalle tue pietre, mute ma loquaci.\\n\\nFontana di Trevi, dove l'acqua scorre\\ne con le sue cascate incanta l'orecchio,\\nPiazza Navona, che d'arte si colora,\\nsono i luoghi dove più mi specchio. \\n\\nRoma, tu sei musa e fonte d'ispirazione,\\nscrigno di meraviglie e cap\"]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([{\"city\": \"Napoli\"}, {\"city\": \"Roma\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = ChatPromptTemplate.from_template(\"Componi una poesia su {topic}\") | model\n",
    "chain2 = (\n",
    "    ChatPromptTemplate.from_template(\"Scrivi un possibile slogan per pubblicizzare i giochi olimpici nella città di {topic}\")\n",
    "    | model\n",
    ")\n",
    "combined = RunnableParallel(poem=chain1, slogan=chain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'poem': ' Ecco la mia poesia su Napoli:\\n\\nNapoli bella, città del sole  \\nTra il mare e il Vesuvio, regina del golfo  \\nLe tue strade brulicano di vita  \\nI tuoi vicoli raccontano storie\\n\\nPizza margherita, profumo di sfogliatelle  \\nVoci che si rincorrono tra i quartieri  \\nIl caffè bollente scalda i cuori  \\nLa gente si incontra, chiacchiera e ride\\n\\nMare blu cobalto, lungomare affollato  \\nIl sole cocente scalda la pietra  \\nGli scugnizzi giocano tra i vicoli  \\nLe nonne cuciono sul balcone  \\n\\nNapoli mia, quanta vita pulsante  \\nTra luci e ombre, gioia e dolore  \\nCittà passionale, dal cuore immenso  \\nNapoli bella, ti porto nel cuore',\n",
       " 'slogan': ' Ecco una proposta di slogan per pubblicizzare i Giochi Olimpici a Napoli:\\n\\n\"Napoli 20XX, dove lo sport incontra la passione\"'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.invoke({\"topic\": \"Napoli\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelism on batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'poem': ' Ecco una poesia su Roma composta da me:\\n\\nRoma eterna, città millenaria\\nDalle rovine parlanti di storia antica\\nLe vie lastricate risuonano i passi\\nDi imperatori, poeti e santi\\n\\nIl Colosseo svetta maestoso \\nTestimone di lotte e di gloria\\nFontana di Trevi luccica al chiaro di luna\\nDesideri esauditi per chi vi getta una moneta\\n\\nLa Città del Vaticano e la Basilica di San Pietro\\nCuore pulsante della cristianità  \\nPiazza Navona gremita di turisti\\nChe ammirano Bernini nel suo capolavoro\\n\\nRoma, culla di civiltà e cultura\\nOgni angolo nasconde un tesoro\\nLa tua bellezza supera i secoli\\nGioiello senza tempo di storia e di arte',\n",
       "  'slogan': ' Ecco una proposta di slogan per pubblicizzare i Giochi Olimpici a Roma:\\n\\n\"Roma 2024, dove la storia incontra il futuro. Vivi l\\'emozione delle Olimpiadi nella Città Eterna!\"'},\n",
       " {'poem': ' Ecco la mia poesia su Milano:\\n\\nMilano, città della moda e del design\\nTra grattacieli che sfiorano il cielo\\nVivi e frenetica, sempre in movimento\\nCon la tua eleganza che non ha eguali\\n\\nDuomo imponente, simbolo indiscusso\\nGalleria, salotto di arte e cultura  \\nTeatri e musei, ricchi di storia e fascino\\nLocali e ristoranti, gioia per il palato\\n\\nTraffico, smog e folla per le strade\\nMa anche parchi verdi, oasi di pace\\nLa tua anima è complessa e affascinante  \\nMilano, non smetterò mai di amarti',\n",
       "  'slogan': ' Ecco una proposta di slogan per pubblicizzare i Giochi Olimpici a Milano:\\n\\n\"Milano 2032, l\\'Olimpiade dell\\'eccellenza italiana\"'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.batch([{\"topic\": \"Roma\"}, {\"topic\": \"Milano\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=model,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Nice and from the sun?\n",
      "AI:  I'm afraid I don't have enough context to understand what you mean by \"Nice and from the sun.\" Could you please rephrase or provide more details about what you're asking? I'd be happy to try answering again with some more context.\n",
      "Human: Nice to meet you. How many km are there between Earth and Mars?\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EGRIMAAY3\\OneDrive - NTT DATA EMEAL\\Desktop\\antonio\\progetti\\2023\\sky\\workshop\\aws-workshop-genai\\.venv\\Lib\\site-packages\\langchain_community\\llms\\bedrock.py:73: UserWarning: Error: Prompt must alternate between '\n",
      "\n",
      "Human:' and '\n",
      "\n",
      "Assistant:'. Received The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Nice and from the sun?\n",
      "AI:  I'm afraid I don't have enough context to understand what you mean by \"Nice and from the sun.\" Could you please rephrase or provide more details about what you're asking? I'd be happy to try answering again with some more context.\n",
      "\n",
      "Human: Nice to meet you. How many km are there between Earth and Mars?\n",
      "AI:\n",
      "\n",
      "Assistant:\n",
      "  warnings.warn(ALTERNATION_ERROR + f\" Received {input_text}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' There is an average distance of about 225 million km (140 million miles) between Earth and Mars. The actual distance varies as the planets orbit the Sun, but this is the approximate mean distance.\\n\\nSome key facts about the distance between Earth and Mars:\\n\\n- The minimum distance is around 54.6 million km when Mars is closest to Earth (at opposition). \\n\\n- The maximum distance is about 401 million km when the planets are farthest apart.\\n\\n- When Mars is closest to Earth, it takes a radio signal 3 minutes to travel between the planets. At the farthest point, it takes about 22 minutes.\\n\\n- The planets orbit the Sun at different speeds, so the distance is constantly changing over time.\\n\\n- Spacecraft launching from Earth to Mars take anywhere from 6-9 months for the journey, depending on the specific trajectory.\\n\\nSo in summary, the average distance is about 225 million km, but it does vary between around 55 million km at the closest point to 401 million km at the farthest point as the two planets orbit the Sun. Let me know if you need any clarification or additional details!'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Nice to meet you. How many km are there between Earth and Mars?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Nice and from the sun?\n",
      "AI:  I'm afraid I don't have enough context to understand what you mean by \"Nice and from the sun.\" Could you please rephrase or provide more details about what you're asking? I'd be happy to try answering again with some more context.\n",
      "Human: Nice to meet you. How many km are there between Earth and Mars?\n",
      "AI:  There is an average distance of about 225 million km (140 million miles) between Earth and Mars. The actual distance varies as the planets orbit the Sun, but this is the approximate mean distance.\n",
      "\n",
      "Some key facts about the distance between Earth and Mars:\n",
      "\n",
      "- The minimum distance is around 54.6 million km when Mars is closest to Earth (at opposition). \n",
      "\n",
      "- The maximum distance is about 401 million km when the planets are farthest apart.\n",
      "\n",
      "- When Mars is closest to Earth, it takes a radio signal 3 minutes to travel between the planets. At the farthest point, it takes about 22 minutes.\n",
      "\n",
      "- The planets orbit the Sun at different speeds, so the distance is constantly changing over time.\n",
      "\n",
      "- Spacecraft launching from Earth to Mars take anywhere from 6-9 months for the journey, depending on the specific trajectory.\n",
      "\n",
      "So in summary, the average distance is about 225 million km, but it does vary between around 55 million km at the closest point to 401 million km at the farthest point as the two planets orbit the Sun. Let me know if you need any clarification or additional details!\n",
      "Human: Nice and from the sun?\n",
      "AI:  I'm afraid I don't fully understand the question \"Nice and from the sun?\". That phrase doesn't give me enough context to provide a substantive response. Could you please rephrase the question or provide some more details about what you are asking? I'd be happy to try answering again if I have a clearer understanding of what information you are looking for.\n",
      "Human: what distance from the sun?\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EGRIMAAY3\\OneDrive - NTT DATA EMEAL\\Desktop\\antonio\\progetti\\2023\\sky\\workshop\\aws-workshop-genai\\.venv\\Lib\\site-packages\\langchain_community\\llms\\bedrock.py:73: UserWarning: Error: Prompt must alternate between '\n",
      "\n",
      "Human:' and '\n",
      "\n",
      "Assistant:'. Received The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Nice and from the sun?\n",
      "AI:  I'm afraid I don't have enough context to understand what you mean by \"Nice and from the sun.\" Could you please rephrase or provide more details about what you're asking? I'd be happy to try answering again with some more context.\n",
      "\n",
      "Human: Nice to meet you. How many km are there between Earth and Mars?\n",
      "AI:  There is an average distance of about 225 million km (140 million miles) between Earth and Mars. The actual distance varies as the planets orbit the Sun, but this is the approximate mean distance.\n",
      "\n",
      "Some key facts about the distance between Earth and Mars:\n",
      "\n",
      "- The minimum distance is around 54.6 million km when Mars is closest to Earth (at opposition). \n",
      "\n",
      "- The maximum distance is about 401 million km when the planets are farthest apart.\n",
      "\n",
      "- When Mars is closest to Earth, it takes a radio signal 3 minutes to travel between the planets. At the farthest point, it takes about 22 minutes.\n",
      "\n",
      "- The planets orbit the Sun at different speeds, so the distance is constantly changing over time.\n",
      "\n",
      "- Spacecraft launching from Earth to Mars take anywhere from 6-9 months for the journey, depending on the specific trajectory.\n",
      "\n",
      "So in summary, the average distance is about 225 million km, but it does vary between around 55 million km at the closest point to 401 million km at the farthest point as the two planets orbit the Sun. Let me know if you need any clarification or additional details!\n",
      "\n",
      "Human: Nice and from the sun?\n",
      "AI:  I'm afraid I don't fully understand the question \"Nice and from the sun?\". That phrase doesn't give me enough context to provide a substantive response. Could you please rephrase the question or provide some more details about what you are asking? I'd be happy to try answering again if I have a clearer understanding of what information you are looking for.\n",
      "\n",
      "Human: what distance from the sun?\n",
      "AI:\n",
      "\n",
      "Assistant:\n",
      "  warnings.warn(ALTERNATION_ERROR + f\" Received {input_text}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Based on the context of our previous discussion about the distance between Earth and Mars, I assume you are now asking about the average distances of Earth and Mars from the Sun.\\n\\nThe average distances from the Sun are:\\n\\n- Earth: 149.6 million km (92.96 million miles)\\n\\n- Mars: 227.9 million km (141.6 million miles) \\n\\nSome key facts:\\n\\n- Earth's distance from the Sun varies from 147.1 million km at perihelion (closest point) to 152.1 million km at aphelion (farthest point).\\n\\n- Mars' distance from the Sun varies from 206.7 million km at perihelion to 249.2 million km at aphelion.\\n\\n- The orbits of both planets are elliptical rather than perfect circles.\\n\\n- Earth is closer to the Sun than Mars is on average. This is why Earth is warmer and Mars has a colder climate.\\n\\nPlease let me know if you need any clarification or have additional questions! I'm happy to provide more details about the distances between the Sun, Earth and Mars.\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"what distance from the sun?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot with Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "memory.chat_memory.add_user_message(\"Context:You will be acting as an expert fitness coach. Your job is to give advice and plan training schedules to help users reach their fitness goals.\")\n",
    "memory.chat_memory.add_ai_message(\"I am an expert fitness coach and give gym advice\")\n",
    "conversation = ConversationChain(\n",
    "    llm=model,\n",
    "    verbose=False,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are some tips to help you train for and compete in a powerlifting competition:\n",
      "\n",
      "- Focus on the three main lifts - squat, bench press, and deadlift. Make sure your technique on these lifts is solid before adding a lot of weight. Work on form with lighter weights first.\n",
      "\n",
      "- Follow a program specifically designed for powerlifting. Look for ones that focus on progressively overloading the main lifts with heavy weights and lower reps. Many programs will also incorporate accessory exercises to build overall strength. \n",
      "\n",
      "- Train the main lifts at least twice per week. Squat and deadlift 1-2 times per week each. Bench press can be trained 2-3 times per week. \n",
      "\n",
      "- Include accessory exercises like overhead press, rows, pull ups, dips, lunges, etc. These help build overall strength to support the main lifts.\n",
      "\n",
      "- Do most sets in the 1-5 rep range on the big lifts. This builds strength and power. Higher reps can be used on accessory lifts.\n",
      "\n",
      "- Take sufficient rest between sets (3-5 mins) when lifting heavy to recover.\n",
      "\n",
      "- Eat enough calories and protein (0.8-1g per pound bodyweight) to support muscle growth and\n"
     ]
    }
   ],
   "source": [
    "print(conversation.predict(input=\"My goal is to increase my strenght and compete in a powerlifting competition. Can you help me with my workout?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EGRIMAAY3\\OneDrive - NTT DATA EMEAL\\Desktop\\antonio\\progetti\\2023\\sky\\workshop\\aws-workshop-genai\\.venv\\Lib\\site-packages\\langchain_community\\llms\\bedrock.py:73: UserWarning: Error: Prompt must alternate between '\n",
      "\n",
      "Human:' and '\n",
      "\n",
      "Assistant:'. Received The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: My goal is to increase my strenght and compete in a powerlifting competition. Can you help me with my workout?\n",
      "AI:  Here are some tips to help you train for and compete in a powerlifting competition:\n",
      "\n",
      "- Focus on the three main lifts - squat, bench press, and deadlift. Make sure your technique on these lifts is solid before adding a lot of weight. Work on form with lighter weights first.\n",
      "\n",
      "- Follow a program specifically designed for powerlifting. Look for ones that focus on progressively overloading the main lifts with heavy weights and lower reps. Many programs will also incorporate accessory exercises to build overall strength. \n",
      "\n",
      "- Train the main lifts at least twice per week. Squat and deadlift 1-2 times per week each. Bench press can be trained 2-3 times per week. \n",
      "\n",
      "- Include accessory exercises like overhead press, rows, pull ups, dips, lunges, etc. These help build overall strength to support the main lifts.\n",
      "\n",
      "- Do most sets in the 1-5 rep range on the big lifts. This builds strength and power. Higher reps can be used on accessory lifts.\n",
      "\n",
      "- Take sufficient rest between sets (3-5 mins) when lifting heavy to recover.\n",
      "\n",
      "- Eat enough calories and protein (0.8-1g per pound bodyweight) to support muscle growth and\n",
      "\n",
      "Human: Ok, but what about progression in every lift week to week?\n",
      "AI:\n",
      "\n",
      "Assistant:\n",
      "  warnings.warn(ALTERNATION_ERROR + f\" Received {input_text}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here are some tips on weekly progression for powerlifting training:\n",
      "\n",
      "- Increase weight on the bar gradually each week to drive progression. Aim for 2.5-10 lbs each week on main lifts.\n",
      "\n",
      "- Use submaximal weights for multiple sets of 1-5 reps. For example, do 3 sets of 3 reps at around 85% of your 1 rep max. Add weight weekly.\n",
      "\n",
      "- Alternate heavy and light days for the main lifts. Heavy days use heavier weights for lower reps. Light days use lighter weights for 5-8 reps. \n",
      "\n",
      "- On light days, focus on speed and technique over weight lifted. This allows recovery from heavy days.\n",
      "\n",
      "- Periodize your training into mesocycles of 4-8 weeks. Progressively increase weight each mesocycle while lowering volume. Then reset with higher volume and lower weights.\n",
      "\n",
      "- Incorporate periodized or programmed deload weeks every 4-8 weeks to allow recovery. Reduce volume and intensity during these weeks. \n",
      "\n",
      "- Monitor rate of progression each week and adjust as needed. If lifts stall for 2-3 weeks, try changing rep ranges, increasing calories, improving form, etc.\n",
      "\n",
      "- Use accessories to bring up weak points. Change accessories\n"
     ]
    }
   ],
   "source": [
    "print(conversation.predict(input=\"Ok, but what about progression in every lift week to week?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation.predict(input=\"Ok, please use kg not lb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
