{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Client Setup [local]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installa la AWS CLI scaricandola da questo url: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html#getting-started-install-instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una volta installata, incolla le informazioni dalla Command line or programmatic access nella folder .aws nel profilo utente di sistema.\n",
    "\n",
    "Nel file config\n",
    "```json\n",
    "[username]\n",
    "region = us-west-2\n",
    "output = json\n",
    "```\n",
    "\n",
    "Nel file credential inserire le informazioni in questo formato:\n",
    "\n",
    "```json\n",
    "[username]\n",
    "aws_access_key_id=\n",
    "aws_secret_access_key=\n",
    "aws_session_token=\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-west-2\"  # E.g. \"us-east-1\"\n",
    "os.environ[\"AWS_PROFILE\"] = \"[fillthis]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from utils import bedrock\n",
    "from langchain.llms import Bedrock\n",
    "\n",
    "\n",
    "bedrock_runtime = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    runtime=True\n",
    ")\n",
    "\n",
    "model = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\", \n",
    "    client=bedrock_runtime,\n",
    "    model_kwargs={'temperature': 0.5}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Client Setup [AWS LAB]\n",
    "\n",
    "Fai il Run di queste celle se sei in ambiente AWS LAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from langchain.llms import Bedrock\n",
    "from utils import bedrock\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "\n",
    "bedrock_runtime = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    runtime=True\n",
    ")\n",
    "\n",
    "model = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\", \n",
    "    client=bedrock_runtime,\n",
    "    model_kwargs={'temperature': 0.5}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test del client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.invoke(\"Componi una poesia sulla città di Napoli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain mette a disposizione una interfaccia agnostica rispetto al LLM che decidiamo di utilizzare. \n",
    "I metodi utilizzabili sono:\n",
    "- stream\n",
    "- invoke\n",
    "- batch\n",
    "\n",
    "Ci sono anche i corrispondenti metodi async che saranno noti a chi conosce python. Per ora li lasceremo da parte.\n",
    "\n",
    "Questi metodi possono essere utilizzati tramite il LangChain Expression Language (LCEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain ci mette a disposizioni diversi metodi di utilità per costruire il nostro prompt in maniera facile e intuitiva. I più semplici sono:\n",
    "- from_template -> Costruisci direttamente da un template riportando le variabili da passare tra parentesi {}\n",
    "- from_messages -> Mantieni la struttura di chat, evidenziando quale stakeholder sta riportando quella interazione (system, human, ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Componi una poesia sulla seguente città: {city}\")\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"city\":\"Roma\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\",\"Sei un cantante adori comppor canzoni ma odi comporre poesie. Se l'utente ti chiede di comporre poesie rispondi in modo sgarbato\"),\n",
    "    (\"human\",\"Componi una poesia riguardo la seguente città: {city}\")\n",
    "]\n",
    "prompt = prompt = ChatPromptTemplate.from_messages(messages=messages)\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.output_schema.schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stream\n",
    "Invoca la generazione e ritorna i chunk man mano che sono generati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\",\"Sei poeta che si ispira al Dolce Stil Novo\"),\n",
    "    (\"human\",\"Componi una poesia riguardo la seguente città: {city}\")\n",
    "]\n",
    "prompt = prompt = ChatPromptTemplate.from_messages(messages=messages)\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in chain.stream({\"city\": \"Biella\"}):\n",
    "    print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke\n",
    "Invoca LLM con i parametri inviati e restituisce la risposta solo a fine elaborazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"city\": \"Milano\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch\n",
    "Permette di invocare la stessa chain in parallelo su parametri diversi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.batch([{\"city\": \"Napoli\"}, {\"city\": \"Roma\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = ChatPromptTemplate.from_template(\"Componi una poesia su {topic}\") | model\n",
    "chain2 = (\n",
    "    ChatPromptTemplate.from_template(\"Scrivi un possibile slogan per pubblicizzare i giochi olimpici nella città di {topic}\")\n",
    "    | model\n",
    ")\n",
    "combined = RunnableParallel(poem=chain1, slogan=chain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.invoke({\"topic\": \"Napoli\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelism on batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.batch([{\"topic\": \"Roma\"}, {\"topic\": \"Milano\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Interface\n",
    "\n",
    "Se si vuole interagire con LLM mantenendo una struttura di Chat, e dunque memoria delle interazioni, Langchain mette a disposizione dei componenti di tipo Memory.\n",
    "Alcuni tipi sono:\n",
    " - ConversationalBuffer: Mantiene la history delle interazioni\n",
    " - ConversationalBufferWithWindows: Mantiene solo le ultime K interazioni\n",
    " - ConversationSummary: Crea una sintesi della conversazione avuta fino ad ora, riducendone la dimensionalità totale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=model,\n",
    "    verbose=True,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Nice to meet you. How many km are there between Earth and Mars?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"what distance from the sun?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot with Role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "memory.chat_memory.add_user_message(\"Context:You will be acting as an expert fitness coach. Your job is to give advice and plan training schedules to help users reach their fitness goals.\")\n",
    "memory.chat_memory.add_ai_message(\"I am an expert fitness coach and give gym advice\")\n",
    "conversation = ConversationChain(\n",
    "    llm=model,\n",
    "    verbose=False,\n",
    "    memory=ConversationBufferMemory()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation.predict(input=\"My goal is to increase my strenght and compete in a powerlifting competition. Can you help me with my workout?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation.predict(input=\"Ok, but what about progression in every lift week to week?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation.predict(input=\"Ok, please use kg not lb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
