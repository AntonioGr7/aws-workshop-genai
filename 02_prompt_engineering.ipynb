{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Client Setup [Local]\n",
    "Installa la AWS CLI scaricandola da questo url: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html#getting-started-install-instructions\n",
    "\n",
    "Una volta installata, incolla le informazioni dalla Command line or programmatic access nella folder .aws nel profilo utente di sistema.\n",
    "\n",
    "\n",
    "Nel file config\n",
    "```json\n",
    "[username]\n",
    "region = us-west-2\n",
    "output = json\n",
    "```\n",
    "\n",
    "Nel file credential inserire le informazioni in questo formato:\n",
    "\n",
    "```json\n",
    "[username]\n",
    "aws_access_key_id=\n",
    "aws_secret_access_key=\n",
    "aws_session_token=\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-west-2\n",
      "  Using profile: sky\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-west-2.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from utils import bedrock\n",
    "from langchain.llms import Bedrock\n",
    "\n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-west-2\"  # E.g. \"us-east-1\"\n",
    "os.environ[\"AWS_PROFILE\"] = \"[]\"\n",
    "\n",
    "bedrock_runtime = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    runtime=True\n",
    ")\n",
    "\n",
    "model = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\", \n",
    "    client=bedrock_runtime,\n",
    "    model_kwargs={'temperature': 1}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Client Setup [AWS LAB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from langchain.llms import Bedrock\n",
    "from utils import bedrock\n",
    "\n",
    "bedrock_runtime = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    runtime=True\n",
    ")\n",
    "\n",
    "model = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\", \n",
    "    client=bedrock_runtime,\n",
    "    model_kwargs={'temperature': 0.5}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering\n",
    "Prompt engineering è una disciplina relativamente nuova che consente di sviluppare e ottimizzare i prompt per utilizzare in modo efficiente i modelli linguistici (LM) per un'ampia varietà di applicazioni e argomenti di ricerca. Le competenze di ingegneria dei prompt aiutano a comprendere meglio le capacità e i limiti dei modelli linguistici di grandi dimensioni (LLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Shot\n",
    "Gli Instruct LLM sono addestrati già di base per seguire le istruzioni. Spesso senza iniettare alcuna informazione ulteriore sono già in grado di comportarsi bene e rispondere alle nostre richieste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Classify the input text in one of the following class: ['negative','neutral','positive'].\n",
    "Input: {text}\n",
    "Sentiment: \n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Okay, let\\'s analyze this:\\n\\nThe input contains several negative phrases like \"the trainer is really bad\" and \"I\\'m in the worse shape of my life\". These suggest the experience being described is quite negative.\\n\\nTherefore, I would classify this text as \\'negative\\'.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"text\":\"the trainer is really bad. I'm in the worse shape of my life\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se vogliamo solo la classe in output dobbiamo essere più espliciti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Classify the input text in one of the following class: ['negative','neutral','positive'].\n",
    "Just give me the sentiment in output and nothing else.\n",
    "Input: {text}\n",
    "Sentiment: \n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' negative'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"text\":\"the trainer is really bad. I'm in the worse shape of my life\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voglio comunque sfruttare il ragionamento che il modello ha fatto, con una catena sequenziale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_1 = \"\"\"Classify the input text in one of the following class: ['negative','neutral','positive'].\n",
    "Input: {text}\n",
    "Think step by step\n",
    "Sentiment: \n",
    "\"\"\"\n",
    "\n",
    "template_2 = \"\"\"Given the following explanation given in output by an LLM, classify the input text in one of the following class: ['negative','neutral','positive'].\n",
    "Just give me the sentiment in output and nothing else.\n",
    "Input: {reasoning}\n",
    "Output: \n",
    "\"\"\"\n",
    "\n",
    "prompt_1 = ChatPromptTemplate.from_template(template=template_1)\n",
    "chain_1 = prompt_1 | model\n",
    "\n",
    "prompt_2 = ChatPromptTemplate.from_template(template=template_2)\n",
    "chain_2 = ({\"reasoning\":chain_1} | prompt_2 | model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"text\": \"you are on average\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"text\": \"you are on average\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"chat\",\n",
      "    \"ChatPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"Classify the input text in one of the following class: ['negative','neutral','positive'].\\nInput: you are on average\\nThink step by step\\nSentiment: \\n\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:Bedrock] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Classify the input text in one of the following class: ['negative','neutral','positive'].\\nInput: you are on average\\nThink step by step\\nSentiment:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:Bedrock] [7.61s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" Okay, let's break this down step-by-step:\\n\\n1. The input text is: \\\"you are on average\\\"\\n2. This is a short text without much context. The words themselves are neutral.\\n3. \\\"You\\\" refers to an unspecified person. Without more context, this is neutral.\\n4. \\\"Are\\\" is a linking verb, neutral. \\n5. \\\"On average\\\" is an idiomatic expression meaning ordinary or typical. This implies neither positive nor negative sentiment.\\n\\nBased on this analysis, I would classify the input text as 'neutral' since there are no clearly positive or negative indicators. The words themselves are neutral and the idiom \\\"on average\\\" implies a typical or ordinary state, not positive or negative. Therefore, my classification is:\\n\\nSentiment: neutral\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [7.61s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \" Okay, let's break this down step-by-step:\\n\\n1. The input text is: \\\"you are on average\\\"\\n2. This is a short text without much context. The words themselves are neutral.\\n3. \\\"You\\\" refers to an unspecified person. Without more context, this is neutral.\\n4. \\\"Are\\\" is a linking verb, neutral. \\n5. \\\"On average\\\" is an idiomatic expression meaning ordinary or typical. This implies neither positive nor negative sentiment.\\n\\nBased on this analysis, I would classify the input text as 'neutral' since there are no clearly positive or negative indicators. The words themselves are neutral and the idiom \\\"on average\\\" implies a typical or ordinary state, not positive or negative. Therefore, my classification is:\\n\\nSentiment: neutral\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Okay, let\\'s break this down step-by-step:\\n\\n1. The input text is: \"you are on average\"\\n2. This is a short text without much context. The words themselves are neutral.\\n3. \"You\" refers to an unspecified person. Without more context, this is neutral.\\n4. \"Are\" is a linking verb, neutral. \\n5. \"On average\" is an idiomatic expression meaning ordinary or typical. This implies neither positive nor negative sentiment.\\n\\nBased on this analysis, I would classify the input text as \\'neutral\\' since there are no clearly positive or negative indicators. The words themselves are neutral and the idiom \"on average\" implies a typical or ordinary state, not positive or negative. Therefore, my classification is:\\n\\nSentiment: neutral'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_1.invoke({\"text\":\"you are on average\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"text\": \"you are on average\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<reasoning>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"text\": \"you are on average\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<reasoning> > 3:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"text\": \"you are on average\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<reasoning> > 3:chain:RunnableSequence > 4:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"text\": \"you are on average\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<reasoning> > 3:chain:RunnableSequence > 4:prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"chat\",\n",
      "    \"ChatPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"Classify the input text in one of the following class: ['negative','neutral','positive'].\\nInput: you are on average\\nThink step by step\\nSentiment: \\n\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<reasoning> > 3:chain:RunnableSequence > 5:llm:Bedrock] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Classify the input text in one of the following class: ['negative','neutral','positive'].\\nInput: you are on average\\nThink step by step\\nSentiment:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<reasoning> > 3:chain:RunnableSequence > 5:llm:Bedrock] [7.15s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" Okay, let's break this down step-by-step:\\n\\n1. The input text is: \\\"you are on average\\\"\\n\\n2. This is a simple statement without any obvious sentiment words. The word \\\"average\\\" on its own is neutral.\\n\\n3. The phrase \\\"you are\\\" is also neutral. It doesn't imply any positive or negative sentiment.\\n\\n4. Looking at the overall input, it is making a neutral statement about someone being average. There are no clear indicators of positive or negative sentiment.\\n\\n5. Therefore, I would classify this input as 'neutral'.\\n\\nSo in summary, by analyzing the input text word-by-word, considering the overall meaning, and lack of clear sentiment clues, I conclude the sentiment is 'neutral'.\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<reasoning> > 3:chain:RunnableSequence] [7.16s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \" Okay, let's break this down step-by-step:\\n\\n1. The input text is: \\\"you are on average\\\"\\n\\n2. This is a simple statement without any obvious sentiment words. The word \\\"average\\\" on its own is neutral.\\n\\n3. The phrase \\\"you are\\\" is also neutral. It doesn't imply any positive or negative sentiment.\\n\\n4. Looking at the overall input, it is making a neutral statement about someone being average. There are no clear indicators of positive or negative sentiment.\\n\\n5. Therefore, I would classify this input as 'neutral'.\\n\\nSo in summary, by analyzing the input text word-by-word, considering the overall meaning, and lack of clear sentiment clues, I conclude the sentiment is 'neutral'.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<reasoning>] [7.16s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"reasoning\": \" Okay, let's break this down step-by-step:\\n\\n1. The input text is: \\\"you are on average\\\"\\n\\n2. This is a simple statement without any obvious sentiment words. The word \\\"average\\\" on its own is neutral.\\n\\n3. The phrase \\\"you are\\\" is also neutral. It doesn't imply any positive or negative sentiment.\\n\\n4. Looking at the overall input, it is making a neutral statement about someone being average. There are no clear indicators of positive or negative sentiment.\\n\\n5. Therefore, I would classify this input as 'neutral'.\\n\\nSo in summary, by analyzing the input text word-by-word, considering the overall meaning, and lack of clear sentiment clues, I conclude the sentiment is 'neutral'.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 6:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"reasoning\": \" Okay, let's break this down step-by-step:\\n\\n1. The input text is: \\\"you are on average\\\"\\n\\n2. This is a simple statement without any obvious sentiment words. The word \\\"average\\\" on its own is neutral.\\n\\n3. The phrase \\\"you are\\\" is also neutral. It doesn't imply any positive or negative sentiment.\\n\\n4. Looking at the overall input, it is making a neutral statement about someone being average. There are no clear indicators of positive or negative sentiment.\\n\\n5. Therefore, I would classify this input as 'neutral'.\\n\\nSo in summary, by analyzing the input text word-by-word, considering the overall meaning, and lack of clear sentiment clues, I conclude the sentiment is 'neutral'.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 6:prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"chat\",\n",
      "    \"ChatPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"Given the following explanation given in output by an LLM, classify the input text in one of the following class: ['negative','neutral','positive'].\\nJust give me the sentiment in output and nothing else.\\nInput:  Okay, let's break this down step-by-step:\\n\\n1. The input text is: \\\"you are on average\\\"\\n\\n2. This is a simple statement without any obvious sentiment words. The word \\\"average\\\" on its own is neutral.\\n\\n3. The phrase \\\"you are\\\" is also neutral. It doesn't imply any positive or negative sentiment.\\n\\n4. Looking at the overall input, it is making a neutral statement about someone being average. There are no clear indicators of positive or negative sentiment.\\n\\n5. Therefore, I would classify this input as 'neutral'.\\n\\nSo in summary, by analyzing the input text word-by-word, considering the overall meaning, and lack of clear sentiment clues, I conclude the sentiment is 'neutral'.\\nOutput: \\n\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 7:llm:Bedrock] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Given the following explanation given in output by an LLM, classify the input text in one of the following class: ['negative','neutral','positive'].\\nJust give me the sentiment in output and nothing else.\\nInput:  Okay, let's break this down step-by-step:\\n\\n1. The input text is: \\\"you are on average\\\"\\n\\n2. This is a simple statement without any obvious sentiment words. The word \\\"average\\\" on its own is neutral.\\n\\n3. The phrase \\\"you are\\\" is also neutral. It doesn't imply any positive or negative sentiment.\\n\\n4. Looking at the overall input, it is making a neutral statement about someone being average. There are no clear indicators of positive or negative sentiment.\\n\\n5. Therefore, I would classify this input as 'neutral'.\\n\\nSo in summary, by analyzing the input text word-by-word, considering the overall meaning, and lack of clear sentiment clues, I conclude the sentiment is 'neutral'.\\nOutput:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 7:llm:Bedrock] [914ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" neutral\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [8.08s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \" neutral\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' neutral'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_2.invoke({\"text\":\"you are on average\"},config={'callbacks': [ConsoleCallbackHandler()]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Branching and Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo combinare le catene in varie sequenze, per ottenere output creativi e completi.\n",
    "\n",
    "In questo esempio chiediamo a due \"agenti\" in parallelo di generare una lista di aspetti positivi e negativi riguardo un certo argomento. Un altro agente prende entrambi i contributi e li usa per generare un report con considerazioni finali. \n",
    "\n",
    "- Planner: Definisce il compito e passa l'informazione agli agenti\n",
    "- Agent Pos: Genera una lista di aspetti positivi (in parallelo a quelli negativi)\n",
    "- Agent Neg: Genera una lista di aspetti negativi (in parallelo a quello positivi)\n",
    "- Final Responder: Raccogli i contributi e metti insieme i pezzi. \n",
    "\n",
    "La combinazione di agent viene utilizzata in diverse tecniche di prompt engineering come Three of Thoughts e Self Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = (\n",
    "    ChatPromptTemplate.from_template(\"Generate an argument about: {input}\")\n",
    "    | model\n",
    "    | {\"base_response\": RunnablePassthrough()}\n",
    ")\n",
    "arguments_for = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"List the pros or positive aspects of {base_response}\"\n",
    "    )\n",
    "    | model\n",
    ")\n",
    "arguments_against = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"List the cons or negative aspects of {base_response}\"\n",
    "    )\n",
    "    | model\n",
    ")\n",
    "final_responder = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"ai\", \"{original_response}\"),\n",
    "            (\"human\", \"Pros:\\n{results_1}\\n\\nCons:\\n{results_2}\"),\n",
    "            (\"system\", \"Combine pros and cons in an unique statement and make a final summary\"),\n",
    "        ]\n",
    "    )\n",
    "    | model\n",
    ")\n",
    "chain = (\n",
    "    planner\n",
    "    | {\n",
    "        \"results_1\": arguments_for,\n",
    "        \"results_2\": arguments_against,\n",
    "        \"original_response\": itemgetter(\"base_response\"),\n",
    "    }\n",
    "    | final_responder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Here is a balanced assessment combining the pros and cons:\\n\\nWhile the dialogue presents the key differences between Democrats and Republicans in a civil manner, it relies heavily on predictable partisan talking points without engaging substantive policy ideas. The tone of seeking common ground is admirable, yet the fundamental ideological divisions remain unaddressed. This type of good-faith discussion is preferable to hostile partisan attacks, and can represent an incremental step forward. However, overcoming current political dysfunction will require much deeper bilateral understanding and a willingness to acknowledge nuance and compromise. In our complex times, we need political discourse based on empathy, critical thinking, and pragmatic solutions that transcend Labels and stereotypes.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"USA - Democrat vs Republican \"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot\n",
    "Il few shot consente di migliorare e indirizzare il comportamento del modello nel risolvere particolari task fornendogli alcuni esempi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    ")\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo alcuni esempi per il nostro sistema di Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"input\": \"You are ugly\", \"output\": \"1\"},\n",
    "    {\"input\": \"look, it's raining \", \"output\": \"3\"},\n",
    "    {\"input\": \"I love you\", \"output\": \"5\"},\n",
    "    {\"input\": \"I like the way you build this model. Well done!\", \"output\": \"5\"},\n",
    "    {\"input\": \"You are not even able to do this work for me. You are useless\", \"output\": \"1\"},\n",
    "    {\"input\": \"You did an ok job\", \"output\": \"4\"},\n",
    "    {\"input\": \"this is not that great, next time do better please\", \"output\": \"2\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"assistant\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Human: You are ugly\\nAI: 1\\nHuman: look, it's raining \\nAI: 3\\nHuman: I love you\\nAI: 5\\nHuman: I like the way you build this model. Well done!\\nAI: 5\\nHuman: You are not even able to do this work for me. You are useless\\nAI: 1\\nHuman: You did an ok job\\nAI: 4\\nHuman: this is not that great, next time do better please\\nAI: 2\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt.format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "            (\"system\", \"You are a sentiment analysis assistant that give a sentiment between 1 and 5, where 5 means extremely positive and 1 extremely negative. Respect the format in the following example\"),\n",
    "            few_shot_prompt,\n",
    "            (\"human\", \"{text}\"),\n",
    "            (\"assistant\", \"\"),\n",
    "        ]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages=messages)\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EGRIMAAY3\\OneDrive - NTT DATA EMEAL\\Desktop\\antonio\\progetti\\2023\\sky\\workshop\\aws-workshop-genai\\.venv\\Lib\\site-packages\\langchain_community\\llms\\bedrock.py:73: UserWarning: Error: Prompt must alternate between '\n",
      "\n",
      "Human:' and '\n",
      "\n",
      "Assistant:'. Received System: You are a sentiment analysis assistant that give a sentiment between 1 and 5, where 5 means extremely positive and 1 extremely negative. Respect the format in the following example\n",
      "\n",
      "Human: You are ugly\n",
      "AI: 1\n",
      "\n",
      "Human: look, it's raining \n",
      "AI: 3\n",
      "\n",
      "Human: I love you\n",
      "AI: 5\n",
      "\n",
      "Human: I like the way you build this model. Well done!\n",
      "AI: 5\n",
      "\n",
      "Human: You are not even able to do this work for me. You are useless\n",
      "AI: 1\n",
      "\n",
      "Human: You did an ok job\n",
      "AI: 4\n",
      "\n",
      "Human: this is not that great, next time do better please\n",
      "AI: 2\n",
      "\n",
      "Human: I don't like the color of this t-shirt but could be worse\n",
      "AI: \n",
      "\n",
      "Assistant:\n",
      "  warnings.warn(ALTERNATION_ERROR + f\" Received {input_text}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 3'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"text\":\"I don't like the color of this t-shirt but could be worse\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain-Of-Thought"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un'altra tecnica molto nota è quella della Catena del pensiero. Portando il modello a ragionare invece di dare la risposta secca, è stato dimostrato che la probabilità di ottenere una risposta corretta è più alta. Nei modelli moderni la Chain of Thought è incorporata direttamente nella logica dei modelli. Noterete che assegnando a questo modello un problema logico matematico molto spesso iniziarà da solo la generazione con la dicitura \"let\\'s go through this step-by-step\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\", \n",
    "    client=bedrock_runtime,\n",
    "    model_kwargs={'temperature': 1,\"stop_sequences\": []} ## Aumento la temperature per esplorare path diversi\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a logical and arithmetical system that solve complex task. \n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n",
    "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n",
    "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\"\"\"\n",
    "\n",
    "messages = [\n",
    "            (\"system\", template),\n",
    "            (\"human\", \"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\"),\n",
    "            (\"assistant\", \"Let's go through this step-by-step\"),\n",
    "        ]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages=messages)\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Okay, here are the steps to solve this problem:\\n\\n1) Identify all the odd numbers in the group: 15, 5, 13, 7, 1\\n\\n2) Add up the odd numbers: \\n15 + 5 + 13 + 7 + 1 = 41\\n\\n3) Check if the sum of the odd numbers is even or odd. \\n41 is an odd number. \\n\\nTherefore, the statement \"The odd numbers in this group add up to an even number\" is false for this set of numbers.\\n\\nThe answer is: False'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gli LLM possono sbagliare, e anzi lo fanno spesso. Una tecnica per verificare la veridicità della informazioni è quello di gar generare molteplici volte una stessa risposta e poi verificarne la consistenza. \n",
    "\n",
    "![](./images/self_consistency.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\", \n",
    "    client=bedrock_runtime,\n",
    "    model_kwargs={'temperature': 1,\"stop_sequences\": [\"\\nQ:\"]} ## Aumento la temperature per esplorare path diversi\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_1 = \"\"\"Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
    "there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
    "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
    "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\n",
    "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
    "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
    "did Jason give to Denny?\n",
    "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
    "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
    "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
    "he have now?\n",
    "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
    "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
    "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
    "monday to thursday. How many computers are now in the server room?\n",
    "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
    "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
    "The answer is 29.\n",
    "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
    "golf balls did he have at the end of wednesday?\n",
    "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
    "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
    "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "A: She bought 5 bagels for $3 each. This means she spent $15. She has $8 left.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "template_2 = \"\"\"Given the answer to the following question '{text}' and the following provided answers, verify the consistency and give me the final correct answer. If necessary use voting.\n",
    "Answer 1: {ans_1},\n",
    "Answer 2: {ans_2}, \n",
    "Answer 3: {ans_3}\n",
    "Think step by step\n",
    "\"\"\"\n",
    "\n",
    "messages_1 = [\n",
    "            (\"system\", template_1),\n",
    "            (\"human\", \"Q: {text}\"),\n",
    "            (\"assistant\", \"A:\"),\n",
    "        ]\n",
    "\n",
    "prompt_1 = ChatPromptTemplate.from_messages(messages=messages_1)\n",
    "chain_1 = prompt_1 | model\n",
    "\n",
    "messages_2 = [\n",
    "            (\"system\", template_2)\n",
    "        ]\n",
    "\n",
    "prompt_2 = ChatPromptTemplate.from_messages(messages=messages_2)\n",
    "chain_2 = prompt_2 | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"When I was 6 my sister was half my age plus the age of my twin brother divided by 3. Now I’m 70 how old is my sister?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_output = chain_1.batch([{\"text\":text},{\"text\":text},{\"text\":text}])\n",
    "final_output = chain_2.invoke({\"text\":text,\"ans_1\":middle_output[0],\"ans_2\":middle_output[1],\"ans_3\":middle_output[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" * I was 6 years old\\n* My sister was half my age (3 years old) plus my twin brother's age divided by 3\\n* My twin brother and I were the same age (6 years old), so his age divided by 3 is 2 \\n* Therefore, my sister's age was 3 + 2 = 5 years old\\n* I am now 70 years old\\n* When I was 6, my sister was 5\\n* So if I am now 70, my sister must be 70 - 6 + 5 = 69 years old\\n\\nTherefore, the age of my sister now is 69 years old.\",\n",
       " \" Okay, let's break this down step-by-step:\\n\\n* You were 6 years old\\n* Your sister was half your age (half of 6 is 3) plus the age of your twin brother divided by 3\\n* Since you and your twin brother were the same age (6 years old), your sister's age was:\\n   - Half your age: 3 \\n   - Plus your brother's age (6) divided by 3: 2\\n   - So your sister was 3 + 2 = 5 years old\\n\\n* Now you are 70 years old \\n* When you were 6, your sister was 5\\n* You are now 70, so your sister must be 70 - 6 + 5 = 69 years old\\n\\nTherefore, if you are now 70 years old and your sister was half your age plus your twin brother's age divided by 3 when you were 6, your sister is now 69 years old.\",\n",
       " \" Okay, let's break this down step-by-step:\\n\\n* When you were 6, your sister was half your age (6/2 = 3) plus the age of your twin brother (also 6) divided by 3 (6/3 = 2)\\n* So your sister's age was 3 + 2 = 5\\n* You said you are now 70 years old \\n* When you were 6, your sister was 5\\n* There is a 64 year difference between when you were 6 and now (70 - 6 = 64)\\n* So if you aged 64 years, your sister must have also aged 64 years\\n* Your sister was 5 when you were 6\\n* So now your sister is 5 + 64 = 69 years old\\n\\nTherefore, the age of your sister now is 69 years old.\"]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "middle_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Based on the two consistent answers provided, I agree that if I was 6 years old, and my sister was half my age (3 years) plus my twin brother's age (also 6) divided by 3 (2 years), then my sister was 5 years old at that time. \\n\\nNow that I am 70 years old, and I was 6 years old when my sister was 5 years old, she must now be 69 years old (70 - 6 + 5).\\n\\nTherefore, the final correct answer is:\\n\\n69 years old\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
