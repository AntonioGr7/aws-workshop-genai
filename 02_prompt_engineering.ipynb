{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Client Setup [Local]\n",
    "Installa la AWS CLI scaricandola da questo url: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html#getting-started-install-instructions\n",
    "\n",
    "Una volta installata, incolla le informazioni dalla Command line or programmatic access nella folder .aws nel profilo utente di sistema.\n",
    "\n",
    "\n",
    "Nel file config\n",
    "```json\n",
    "[username]\n",
    "region = us-west-2\n",
    "output = json\n",
    "```\n",
    "\n",
    "Nel file credential inserire le informazioni in questo formato:\n",
    "\n",
    "```json\n",
    "[username]\n",
    "aws_access_key_id=\n",
    "aws_secret_access_key=\n",
    "aws_session_token=\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from utils import bedrock\n",
    "from langchain.llms import Bedrock\n",
    "\n",
    "\n",
    "os.environ[\"AWS_DEFAULT_REGION\"] = \"us-west-2\"  # E.g. \"us-east-1\"\n",
    "os.environ[\"AWS_PROFILE\"] = \"[]\"\n",
    "\n",
    "bedrock_runtime = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    runtime=True\n",
    ")\n",
    "\n",
    "model = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\", \n",
    "    client=bedrock_runtime,\n",
    "    model_kwargs={'temperature': 1}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Client Setup [AWS LAB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from langchain.llms import Bedrock\n",
    "from utils import bedrock\n",
    "\n",
    "bedrock_runtime = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None),\n",
    "    runtime=True\n",
    ")\n",
    "\n",
    "model = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\", \n",
    "    client=bedrock_runtime,\n",
    "    model_kwargs={'temperature': 0.5}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering\n",
    "Prompt engineering è una disciplina relativamente nuova che consente di sviluppare e ottimizzare i prompt per utilizzare in modo efficiente i modelli linguistici (LM) per un'ampia varietà di applicazioni e argomenti di ricerca. Le competenze di ingegneria dei prompt aiutano a comprendere meglio le capacità e i limiti dei modelli linguistici di grandi dimensioni (LLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Shot\n",
    "Gli Instruct LLM sono addestrati già di base per seguire le istruzioni. Spesso senza iniettare alcuna informazione ulteriore sono già in grado di comportarsi bene e rispondere alle nostre richieste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Classify the input text in one of the following class: ['negative','neutral','positive'].\n",
    "Input: {text}\n",
    "Sentiment: \n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"text\":\"the trainer is really bad. I'm in the worse shape of my life\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se vogliamo solo la classe in output dobbiamo essere più espliciti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Classify the input text in one of the following class: ['negative','neutral','positive'].\n",
    "Just give me the sentiment in output and nothing else.\n",
    "Input: {text}\n",
    "Sentiment: \n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=template)\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"text\":\"the trainer is really bad. I'm in the worse shape of my life\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voglio comunque sfruttare il ragionamento che il modello ha fatto, con una catena sequenziale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_1 = \"\"\"Classify the input text in one of the following class: ['negative','neutral','positive'].\n",
    "Input: {text}\n",
    "Think step by step\n",
    "Sentiment: \n",
    "\"\"\"\n",
    "\n",
    "template_2 = \"\"\"Given the following explanation given in output by an LLM, classify the input text in one of the following class: ['negative','neutral','positive'].\n",
    "Just give me the sentiment in output and nothing else.\n",
    "Input: {reasoning}\n",
    "Output: \n",
    "\"\"\"\n",
    "\n",
    "prompt_1 = ChatPromptTemplate.from_template(template=template_1)\n",
    "chain_1 = prompt_1 | model\n",
    "\n",
    "prompt_2 = ChatPromptTemplate.from_template(template=template_2)\n",
    "chain_2 = ({\"reasoning\":chain_1} | prompt_2 | model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_1.invoke({\"text\":\"you are on average\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_2.invoke({\"text\":\"you are on average\"},config={'callbacks': [ConsoleCallbackHandler()]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Branching and Merging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo combinare le catene in varie sequenze, per ottenere output creativi e completi.\n",
    "\n",
    "In questo esempio chiediamo a due \"agenti\" in parallelo di generare una lista di aspetti positivi e negativi riguardo un certo argomento. Un altro agente prende entrambi i contributi e li usa per generare un report con considerazioni finali. \n",
    "\n",
    "- Planner: Definisce il compito e passa l'informazione agli agenti\n",
    "- Agent Pos: Genera una lista di aspetti positivi (in parallelo a quelli negativi)\n",
    "- Agent Neg: Genera una lista di aspetti negativi (in parallelo a quello positivi)\n",
    "- Final Responder: Raccogli i contributi e metti insieme i pezzi. \n",
    "\n",
    "La combinazione di agent viene utilizzata in diverse tecniche di prompt engineering come Three of Thoughts e Self Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = (\n",
    "    ChatPromptTemplate.from_template(\"Generate an argument about: {input}\")\n",
    "    | model\n",
    "    | {\"base_response\": RunnablePassthrough()}\n",
    ")\n",
    "arguments_for = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"List the pros or positive aspects of {base_response}\"\n",
    "    )\n",
    "    | model\n",
    ")\n",
    "arguments_against = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"List the cons or negative aspects of {base_response}\"\n",
    "    )\n",
    "    | model\n",
    ")\n",
    "final_responder = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"ai\", \"{original_response}\"),\n",
    "            (\"human\", \"Pros:\\n{results_1}\\n\\nCons:\\n{results_2}\"),\n",
    "            (\"system\", \"Combine pros and cons in an unique statement and make a final summary\"),\n",
    "        ]\n",
    "    )\n",
    "    | model\n",
    ")\n",
    "chain = (\n",
    "    planner\n",
    "    | {\n",
    "        \"results_1\": arguments_for,\n",
    "        \"results_2\": arguments_against,\n",
    "        \"original_response\": itemgetter(\"base_response\"),\n",
    "    }\n",
    "    | final_responder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"USA - Democrat vs Republican \"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few Shot\n",
    "Il few shot consente di migliorare e indirizzare il comportamento del modello nel risolvere particolari task fornendogli alcuni esempi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate,\n",
    ")\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo alcuni esempi per il nostro sistema di Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\"input\": \"You are ugly\", \"output\": \"1\"},\n",
    "    {\"input\": \"look, it's raining \", \"output\": \"3\"},\n",
    "    {\"input\": \"I love you\", \"output\": \"5\"},\n",
    "    {\"input\": \"I like the way you build this model. Well done!\", \"output\": \"5\"},\n",
    "    {\"input\": \"You are not even able to do this work for me. You are useless\", \"output\": \"1\"},\n",
    "    {\"input\": \"You did an ok job\", \"output\": \"4\"},\n",
    "    {\"input\": \"this is not that great, next time do better please\", \"output\": \"2\"},\n",
    "]\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"assistant\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt.format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "            (\"system\", \"You are a sentiment analysis assistant that give a sentiment between 1 and 5, where 5 means extremely positive and 1 extremely negative. Respect the format in the following example\"),\n",
    "            few_shot_prompt,\n",
    "            (\"human\", \"{text}\"),\n",
    "            (\"assistant\", \"\"),\n",
    "        ]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages=messages)\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"text\":\"I don't like the color of this t-shirt but could be worse\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain-Of-Thought"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un'altra tecnica molto nota è quella della Catena del pensiero. Portando il modello a ragionare invece di dare la risposta secca, è stato dimostrato che la probabilità di ottenere una risposta corretta è più alta. Nei modelli moderni la Chain of Thought è incorporata direttamente nella logica dei modelli. Noterete che assegnando a questo modello un problema logico matematico molto spesso iniziarà da solo la generazione con la dicitura \"let\\'s go through this step-by-step\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\", \n",
    "    client=bedrock_runtime,\n",
    "    model_kwargs={'temperature': 1,\"stop_sequences\": []} ## Aumento la temperature per esplorare path diversi\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a logical and arithmetical system that solve complex task. \n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.\n",
    "A: Adding all the odd numbers (17, 19) gives 36. The answer is True.\n",
    "The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.\n",
    "A: Adding all the odd numbers (11, 13) gives 24. The answer is True.\n",
    "The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.\n",
    "A: Adding all the odd numbers (17, 9, 13) gives 39. The answer is False.\"\"\"\n",
    "\n",
    "messages = [\n",
    "            (\"system\", template),\n",
    "            (\"human\", \"The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.\"),\n",
    "            (\"assistant\", \"Let's go through this step-by-step\"),\n",
    "        ]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages=messages)\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gli LLM possono sbagliare, e anzi lo fanno spesso. Una tecnica per verificare la veridicità della informazioni è quello di gar generare molteplici volte una stessa risposta e poi verificarne la consistenza. \n",
    "\n",
    "![](./images/self_consistency.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Bedrock(\n",
    "    model_id=\"anthropic.claude-v2\", \n",
    "    client=bedrock_runtime,\n",
    "    model_kwargs={'temperature': 1,\"stop_sequences\": [\"\\nQ:\"]} ## Aumento la temperature per esplorare path diversi\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_1 = \"\"\"Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done,\n",
    "there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted.\n",
    "So, they must have planted 21 - 15 = 6 trees. The answer is 6.\n",
    "Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\n",
    "A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5.\n",
    "Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
    "A: Leah had 32 chocolates and Leah’s sister had 42. That means there were originally 32 + 42 = 74\n",
    "chocolates. 35 have been eaten. So in total they still have 74 - 35 = 39 chocolates. The answer is 39.\n",
    "Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops\n",
    "did Jason give to Denny?\n",
    "A: Jason had 20 lollipops. Since he only has 12 now, he must have given the rest to Denny. The number of\n",
    "lollipops he has given to Denny must have been 20 - 12 = 8 lollipops. The answer is 8.\n",
    "Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does\n",
    "he have now?\n",
    "A: He has 5 toys. He got 2 from mom, so after that he has 5 + 2 = 7 toys. Then he got 2 more from dad, so\n",
    "in total he has 7 + 2 = 9 toys. The answer is 9.\n",
    "Q: There were nine computers in the server room. Five more computers were installed each day, from\n",
    "monday to thursday. How many computers are now in the server room?\n",
    "A: There are 4 days from monday to thursday. 5 computers were added each day. That means in total 4 * 5 =\n",
    "20 computers were added. There were 9 computers in the beginning, so now there are 9 + 20 = 29 computers.\n",
    "The answer is 29.\n",
    "Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many\n",
    "golf balls did he have at the end of wednesday?\n",
    "A: Michael initially had 58 balls. He lost 23 on Tuesday, so after that he has 58 - 23 = 35 balls. On\n",
    "Wednesday he lost 2 more so now he has 35 - 2 = 33 balls. The answer is 33.\n",
    "Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?\n",
    "A: She bought 5 bagels for $3 each. This means she spent $15. She has $8 left.\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "template_2 = \"\"\"Given the answer to the following question '{text}' and the following provided answers, verify the consistency and give me the final correct answer. If necessary use voting.\n",
    "Answer 1: {ans_1},\n",
    "Answer 2: {ans_2}, \n",
    "Answer 3: {ans_3}\n",
    "Think step by step\n",
    "\"\"\"\n",
    "\n",
    "messages_1 = [\n",
    "            (\"system\", template_1),\n",
    "            (\"human\", \"Q: {text}\"),\n",
    "            (\"assistant\", \"A:\"),\n",
    "        ]\n",
    "\n",
    "prompt_1 = ChatPromptTemplate.from_messages(messages=messages_1)\n",
    "chain_1 = prompt_1 | model\n",
    "\n",
    "messages_2 = [\n",
    "            (\"system\", template_2)\n",
    "        ]\n",
    "\n",
    "prompt_2 = ChatPromptTemplate.from_messages(messages=messages_2)\n",
    "chain_2 = prompt_2 | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"When I was 6 my sister was half my age plus the age of my twin brother divided by 3. Now I’m 70 how old is my sister?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_output = chain_1.batch([{\"text\":text},{\"text\":text},{\"text\":text}])\n",
    "final_output = chain_2.invoke({\"text\":text,\"ans_1\":middle_output[0],\"ans_2\":middle_output[1],\"ans_3\":middle_output[2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
